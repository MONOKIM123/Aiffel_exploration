{"cells":[{"cell_type":"markdown","metadata":{"id":"EvX6Td4aHNUv"},"source":["# 모델 구현 부분까지만 제출합니다."]},{"cell_type":"markdown","metadata":{"id":"LQP3Rp6HHNUx"},"source":["# Resnet34"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"KpBJrKf2HNUy"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\K\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torchsummary\n","import torch.onnx"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"_lxhK7tvHNUz"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","\n","class Block(nn.Module):\n","\n","    def __init__(self, architecture, is_shortcut):\n","        super(Block, self).__init__()\n","        self.num_repeat = architecture[0] # 첫번째 인자가 반복 횟수\n","        self.architecture = architecture[1:] # 나머지 인자가 CNN block 파라미터\n","        self.is_shorcut = is_shortcut\n","        self.shortcut = nn.Sequential()\n","        self.block = self._block(self.num_repeat, self.architecture)\n","        self.relu = nn.ReLU()\n","\n","    \n","    def _block(self, num_repeat, architecture):\n","        layers = []\n","\n","        for i in range(num_repeat):\n","            for j, (in_channels, out_channels, kernel_size, stride, padding) in enumerate(architecture):\n","                layers.append(nn.Conv2d(\n","                    in_channels=in_channels,\n","                    out_channels=out_channels,\n","                    kernel_size=kernel_size,\n","                    stride=stride,\n","                    padding=padding,\n","                    bias=False,\n","                ))\n","                layers.append(nn.BatchNorm2d(out_channels))\n","                # 가장 마지막 ReLU는 identity를 더한 후 적용한다.\n","                if not (((i + 1) == num_repeat) and ((j + 1) == len(architecture))):\n","                    layers.append(nn.ReLU())\n","                    \n","        return nn.Sequential(*layers)\n","\n","\n","    def forward(self, x):\n","        out = self.block(x)\n","        if self.is_shorcut:\n","            identity = self.shortcut(x)\n","            out += identity\n","        out =  self.relu(out)\n","        return out\n","\n","\n","class Resnet34(nn.Module):\n","\n","    def __init__(self):\n","        super(Resnet34, self).__init__()\n","        self.architecture = [\n","            [\"I\", (3, 64, 7, 2, 3)], # [입력층을 의미하는 코드, 적용할 CNN인자]\n","            [\"M\", (3, 2)], # [최대풀링을 의미하는 코드, 적용할 MaxPool인자]\n","            [3, (64, 64, 3, 1, 1), (64, 64, 3, 1, 1)], # [블록 반복횟수, 적용할 CNN인자]\n","            [1, (64, 128, 3, 2, 1)], # [블록 반복횟수(이미지 사이즈 감소), 적용할 CNN인자]\n","            [4, (128, 128, 3, 1, 1), (128, 128, 3, 1, 1)],\n","            [1, (128, 256, 3, 2, 1)],\n","            [6, (256, 256, 3, 1, 1), (256, 256, 3, 1, 1)],\n","            [1, (256, 512, 3, 2, 1)],\n","            [3, (512, 512, 3, 1, 1), (512, 512, 3, 1, 1)],\n","        ]\n","        self.resnet34 = nn.Sequential(\n","            self._block(self.architecture),\n","            nn.AvgPool2d(1, 1),\n","            nn.Flatten(1, -1), # 배치까지 flatten을 적용하면 안되므로 (Batch(0), Channel(1), H(2), W(3)) 1부터 3까지만 flatten 함\n","            nn.Linear(512*7*7, 1),\n","            nn.Sigmoid(),\n","        )\n","\n","    \n","    def _block(self, architecture):\n","        layers = []\n","\n","        for x in architecture:\n","            # x의 첫번째 인자는 block의 종류를 결정\n","            if x[0] == \"I\": # 블록을 쌓는 코드, Input 레이어를 의미\n","                in_channels, out_channels, kernel_size, stride, padding = x[1]\n","                layers.append(nn.Conv2d(\n","                    in_channels=in_channels,\n","                    out_channels=out_channels,\n","                    kernel_size=kernel_size,\n","                    stride=stride,\n","                    padding=padding,\n","                    bias=False,\n","                ))\n","\n","            elif x[0] == \"M\": # MaxPooling 레이어를 의미\n","                kernel_size, stride = x[1]\n","                layers.append(nn.MaxPool2d(\n","                    kernel_size=kernel_size,\n","                    stride=stride,\n","                ))\n","\n","            elif x[0] > 1: # 블록을 쌓는 코드, 블록 반복 횟수를 의미\n","                block_architecture = x\n","                layers.append(Block(block_architecture, is_shortcut=True))\n","\n","            elif x[0]== 1: # 이미지 사이즈를 줄이는 블록을 쌓는 코드, 블록 반복 횟수를 의미, 이때는 shortcut을 적용하지 않음\n","                block_architecture = x\n","                layers.append(Block(block_architecture, is_shortcut=False))\n","                \n","        return nn.Sequential(*layers)\n","\n","\n","    def forward(self, x):\n","        out = self.resnet34(x)\n","        return out"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"BRiOpO1MHNU0"},"outputs":[],"source":["device = 'cuda'\n","model = Resnet34()\n","model = model.to(device)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Bou7JS5CHNU0","outputId":"a9d33fbb-6037-4c00-e0b5-3772598531bd"},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 112, 112]           9,408\n","         MaxPool2d-2           [-1, 64, 55, 55]               0\n","            Conv2d-3           [-1, 64, 55, 55]          36,864\n","       BatchNorm2d-4           [-1, 64, 55, 55]             128\n","              ReLU-5           [-1, 64, 55, 55]               0\n","            Conv2d-6           [-1, 64, 55, 55]          36,864\n","       BatchNorm2d-7           [-1, 64, 55, 55]             128\n","              ReLU-8           [-1, 64, 55, 55]               0\n","            Conv2d-9           [-1, 64, 55, 55]          36,864\n","      BatchNorm2d-10           [-1, 64, 55, 55]             128\n","             ReLU-11           [-1, 64, 55, 55]               0\n","           Conv2d-12           [-1, 64, 55, 55]          36,864\n","      BatchNorm2d-13           [-1, 64, 55, 55]             128\n","             ReLU-14           [-1, 64, 55, 55]               0\n","           Conv2d-15           [-1, 64, 55, 55]          36,864\n","      BatchNorm2d-16           [-1, 64, 55, 55]             128\n","             ReLU-17           [-1, 64, 55, 55]               0\n","           Conv2d-18           [-1, 64, 55, 55]          36,864\n","      BatchNorm2d-19           [-1, 64, 55, 55]             128\n","             ReLU-20           [-1, 64, 55, 55]               0\n","            Block-21           [-1, 64, 55, 55]               0\n","           Conv2d-22          [-1, 128, 28, 28]          73,728\n","      BatchNorm2d-23          [-1, 128, 28, 28]             256\n","             ReLU-24          [-1, 128, 28, 28]               0\n","            Block-25          [-1, 128, 28, 28]               0\n","           Conv2d-26          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-27          [-1, 128, 28, 28]             256\n","             ReLU-28          [-1, 128, 28, 28]               0\n","           Conv2d-29          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-30          [-1, 128, 28, 28]             256\n","             ReLU-31          [-1, 128, 28, 28]               0\n","           Conv2d-32          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-33          [-1, 128, 28, 28]             256\n","             ReLU-34          [-1, 128, 28, 28]               0\n","           Conv2d-35          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-36          [-1, 128, 28, 28]             256\n","             ReLU-37          [-1, 128, 28, 28]               0\n","           Conv2d-38          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-39          [-1, 128, 28, 28]             256\n","             ReLU-40          [-1, 128, 28, 28]               0\n","           Conv2d-41          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-42          [-1, 128, 28, 28]             256\n","             ReLU-43          [-1, 128, 28, 28]               0\n","           Conv2d-44          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-45          [-1, 128, 28, 28]             256\n","             ReLU-46          [-1, 128, 28, 28]               0\n","           Conv2d-47          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-48          [-1, 128, 28, 28]             256\n","             ReLU-49          [-1, 128, 28, 28]               0\n","            Block-50          [-1, 128, 28, 28]               0\n","           Conv2d-51          [-1, 256, 14, 14]         294,912\n","      BatchNorm2d-52          [-1, 256, 14, 14]             512\n","             ReLU-53          [-1, 256, 14, 14]               0\n","            Block-54          [-1, 256, 14, 14]               0\n","           Conv2d-55          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-56          [-1, 256, 14, 14]             512\n","             ReLU-57          [-1, 256, 14, 14]               0\n","           Conv2d-58          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-59          [-1, 256, 14, 14]             512\n","             ReLU-60          [-1, 256, 14, 14]               0\n","           Conv2d-61          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-62          [-1, 256, 14, 14]             512\n","             ReLU-63          [-1, 256, 14, 14]               0\n","           Conv2d-64          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-65          [-1, 256, 14, 14]             512\n","             ReLU-66          [-1, 256, 14, 14]               0\n","           Conv2d-67          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-68          [-1, 256, 14, 14]             512\n","             ReLU-69          [-1, 256, 14, 14]               0\n","           Conv2d-70          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-71          [-1, 256, 14, 14]             512\n","             ReLU-72          [-1, 256, 14, 14]               0\n","           Conv2d-73          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-74          [-1, 256, 14, 14]             512\n","             ReLU-75          [-1, 256, 14, 14]               0\n","           Conv2d-76          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-77          [-1, 256, 14, 14]             512\n","             ReLU-78          [-1, 256, 14, 14]               0\n","           Conv2d-79          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-80          [-1, 256, 14, 14]             512\n","             ReLU-81          [-1, 256, 14, 14]               0\n","           Conv2d-82          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-83          [-1, 256, 14, 14]             512\n","             ReLU-84          [-1, 256, 14, 14]               0\n","           Conv2d-85          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-86          [-1, 256, 14, 14]             512\n","             ReLU-87          [-1, 256, 14, 14]               0\n","           Conv2d-88          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-89          [-1, 256, 14, 14]             512\n","             ReLU-90          [-1, 256, 14, 14]               0\n","            Block-91          [-1, 256, 14, 14]               0\n","           Conv2d-92            [-1, 512, 7, 7]       1,179,648\n","      BatchNorm2d-93            [-1, 512, 7, 7]           1,024\n","             ReLU-94            [-1, 512, 7, 7]               0\n","            Block-95            [-1, 512, 7, 7]               0\n","           Conv2d-96            [-1, 512, 7, 7]       2,359,296\n","      BatchNorm2d-97            [-1, 512, 7, 7]           1,024\n","             ReLU-98            [-1, 512, 7, 7]               0\n","           Conv2d-99            [-1, 512, 7, 7]       2,359,296\n","     BatchNorm2d-100            [-1, 512, 7, 7]           1,024\n","            ReLU-101            [-1, 512, 7, 7]               0\n","          Conv2d-102            [-1, 512, 7, 7]       2,359,296\n","     BatchNorm2d-103            [-1, 512, 7, 7]           1,024\n","            ReLU-104            [-1, 512, 7, 7]               0\n","          Conv2d-105            [-1, 512, 7, 7]       2,359,296\n","     BatchNorm2d-106            [-1, 512, 7, 7]           1,024\n","            ReLU-107            [-1, 512, 7, 7]               0\n","          Conv2d-108            [-1, 512, 7, 7]       2,359,296\n","     BatchNorm2d-109            [-1, 512, 7, 7]           1,024\n","            ReLU-110            [-1, 512, 7, 7]               0\n","          Conv2d-111            [-1, 512, 7, 7]       2,359,296\n","     BatchNorm2d-112            [-1, 512, 7, 7]           1,024\n","            ReLU-113            [-1, 512, 7, 7]               0\n","           Block-114            [-1, 512, 7, 7]               0\n","       AvgPool2d-115            [-1, 512, 7, 7]               0\n","         Flatten-116                [-1, 25088]               0\n","          Linear-117                    [-1, 1]          25,089\n","         Sigmoid-118                    [-1, 1]               0\n","================================================================\n","Total params: 24,234,177\n","Trainable params: 24,234,177\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 78.35\n","Params size (MB): 92.45\n","Estimated Total Size (MB): 171.37\n","----------------------------------------------------------------\n"]}],"source":["torchsummary.summary(model, (3, 224, 224), device='cuda')"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"nNTfHKqOHNU1"},"outputs":[],"source":["input_names = ['Input']\n","output_names = ['Output']\n","\n","x = torch.zeros(2, 3, 224, 224).to(device)\n","torch.onnx.export(model, x, 'Resnet34.onnx', input_names=input_names, output_names=output_names)"]},{"cell_type":"markdown","metadata":{"id":"NG16R2GnHNU1"},"source":["<img src='https://drive.google.com/uc?export=download&id=1jdvzWZLVZ7mwboIssAIpM-RfVg_L2GEM' width=\"\" height =\"\" /><br>\n"]},{"cell_type":"markdown","metadata":{"id":"mAYSInOsHNU2"},"source":["# Resnet50"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"itQ7rSvCHNU2"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","\n","class Block(nn.Module):\n","\n","    def __init__(self, architecture, is_shortcut):\n","        super(Block, self).__init__()\n","        self.num_repeat = architecture[0] # 첫번째 인자가 반복 횟수\n","        self.architecture = architecture[1:] # 나머지 인자가 CNN block 파라미터\n","        self.is_shorcut = is_shortcut\n","        if self.is_shorcut:\n","            # 1x1 Convolution filter\n","            # 마지막 CNN 파라미터의 입출력 필터 사이즈가 shortcut의 입출력 필터 사이즈와 일치함\n","            self.shortcut = nn.Conv2d(\n","                in_channels=architecture[-1][0],\n","                out_channels=architecture[-1][1],\n","                kernel_size=1,\n","                stride=1,\n","                padding=0,\n","                bias=False,\n","            )\n","        self.block = self._block(self.num_repeat, self.architecture)\n","        self.relu = nn.ReLU()\n","\n","    \n","    def _block(self, num_repeat, architecture):\n","        layers = []\n","\n","        in_channels = architecture[0][0]\n","        for i in range(num_repeat):\n","            for j, (_, out_channels, kernel_size, stride, padding) in enumerate(architecture):\n","                layers.append(nn.Conv2d(\n","                    in_channels=in_channels,\n","                    out_channels=out_channels,\n","                    kernel_size=kernel_size,\n","                    stride=stride,\n","                    padding=padding,\n","                    bias=False,\n","                ))\n","                layers.append(nn.BatchNorm2d(out_channels))\n","                # 가장 마지막 ReLU는 identity를 더한 후 적용한다.\n","                if not (((i + 1) == num_repeat) and ((j + 1) == len(architecture))):\n","                    layers.append(nn.ReLU())\n","                # 반복 사이클의 마지막에서 입출력 채널을 맞춰주기 위해 적용\n","                in_channels = out_channels\n","\n","        return nn.Sequential(*layers)\n","\n","\n","    def forward(self, x):\n","        out = self.block(x)\n","        if self.is_shorcut:\n","            identity = self.shortcut(x)\n","            out += identity\n","        out =  self.relu(out)\n","        return out\n","\n","\n","class Resnet50(nn.Module):\n","\n","    def __init__(self):\n","        super(Resnet50, self).__init__()\n","        self.architecture = [\n","            [\"I\", (3, 64, 7, 2, 3)], # [입력층을 의미하는 코드, 적용할 CNN인자]\n","            [\"M\", (3, 2)], # [최대풀링을 의미하는 코드, 적용할 MaxPool인자]\n","            [3, (64, 64, 1, 1, 0), (64, 64, 3, 1, 1), (64, 256, 1, 1, 0)], # [블록 반복횟수, 적용할 CNN인자]\n","            [1, (256, 128, 3, 2, 1)], # [블록 반복횟수(이미지 사이즈 감소), 적용할 CNN인자]\n","            [4, (128, 128, 1, 1, 0), (128, 128, 3, 1, 1), (128, 512, 1, 1, 0)],\n","            [1, (512, 256, 3, 2, 1)],\n","            [6, (256, 256, 1, 1, 0), (256, 256, 3, 1, 1), (256, 1024, 1, 1, 0)],\n","            [1, (1024, 512, 3, 2, 1)],\n","            [3, (512, 512, 1, 1, 0), (512, 512, 3, 1, 1), (512, 2048, 1, 1, 0)],\n","        ]\n","        self.resnet50 = nn.Sequential(\n","            self._block(self.architecture),\n","            nn.AvgPool2d(1, 1),\n","            nn.Flatten(1, -1), # 배치까지 flatten을 적용하면 안되므로 (Batch(0), Channel(1), H(2), W(3)) 1부터 3까지만 flatten 함\n","            nn.Linear(2048*7*7, 1),\n","            nn.Sigmoid(),\n","        )\n","\n","    \n","    def _block(self, architecture):\n","        layers = []\n","\n","        for x in architecture:\n","            # x의 첫번째 인자는 block의 종류를 결정\n","            if x[0] == \"I\": # 블록을 쌓는 코드, Input 레이어를 의미\n","                in_channels, out_channels, kernel_size, stride, padding = x[1]\n","                layers.append(nn.Conv2d(\n","                    in_channels=in_channels,\n","                    out_channels=out_channels,\n","                    kernel_size=kernel_size,\n","                    stride=stride,\n","                    padding=padding,\n","                    bias=False,\n","                ))\n","\n","            elif x[0] == \"M\": # MaxPooling 레이어를 의미\n","                kernel_size, stride = x[1]\n","                layers.append(nn.MaxPool2d(\n","                    kernel_size=kernel_size,\n","                    stride=stride,\n","                ))\n","\n","            elif x[0] > 1: # 블록을 쌓는 코드, 블록 반복 횟수를 의미\n","                block_architecture = x\n","                layers.append(Block(block_architecture, is_shortcut=True))\n","\n","            elif x[0]== 1: # 이미지 사이즈를 줄이는 블록을 쌓는 코드, 블록 반복 횟수를 의미, 이때는 shortcut을 적용하지 않음\n","                block_architecture = x\n","                layers.append(Block(block_architecture, is_shortcut=False))\n","\n","        return nn.Sequential(*layers)\n","\n","\n","    def forward(self, x):\n","        out = self.resnet50(x)\n","        return out"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"24JDZTWyHNU2"},"outputs":[],"source":["device = 'cuda'\n","model = Resnet50()\n","model = model.to(device)"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"icDFs0zpHNU3","outputId":"637bedf1-8a63-47e2-a8d2-cd0084271602"},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 112, 112]           9,408\n","         MaxPool2d-2           [-1, 64, 55, 55]               0\n","            Conv2d-3           [-1, 64, 55, 55]           4,096\n","       BatchNorm2d-4           [-1, 64, 55, 55]             128\n","              ReLU-5           [-1, 64, 55, 55]               0\n","            Conv2d-6           [-1, 64, 55, 55]          36,864\n","       BatchNorm2d-7           [-1, 64, 55, 55]             128\n","              ReLU-8           [-1, 64, 55, 55]               0\n","            Conv2d-9          [-1, 256, 55, 55]          16,384\n","      BatchNorm2d-10          [-1, 256, 55, 55]             512\n","             ReLU-11          [-1, 256, 55, 55]               0\n","           Conv2d-12           [-1, 64, 55, 55]          16,384\n","      BatchNorm2d-13           [-1, 64, 55, 55]             128\n","             ReLU-14           [-1, 64, 55, 55]               0\n","           Conv2d-15           [-1, 64, 55, 55]          36,864\n","      BatchNorm2d-16           [-1, 64, 55, 55]             128\n","             ReLU-17           [-1, 64, 55, 55]               0\n","           Conv2d-18          [-1, 256, 55, 55]          16,384\n","      BatchNorm2d-19          [-1, 256, 55, 55]             512\n","             ReLU-20          [-1, 256, 55, 55]               0\n","           Conv2d-21           [-1, 64, 55, 55]          16,384\n","      BatchNorm2d-22           [-1, 64, 55, 55]             128\n","             ReLU-23           [-1, 64, 55, 55]               0\n","           Conv2d-24           [-1, 64, 55, 55]          36,864\n","      BatchNorm2d-25           [-1, 64, 55, 55]             128\n","             ReLU-26           [-1, 64, 55, 55]               0\n","           Conv2d-27          [-1, 256, 55, 55]          16,384\n","      BatchNorm2d-28          [-1, 256, 55, 55]             512\n","           Conv2d-29          [-1, 256, 55, 55]          16,384\n","             ReLU-30          [-1, 256, 55, 55]               0\n","            Block-31          [-1, 256, 55, 55]               0\n","           Conv2d-32          [-1, 128, 28, 28]         294,912\n","      BatchNorm2d-33          [-1, 128, 28, 28]             256\n","             ReLU-34          [-1, 128, 28, 28]               0\n","            Block-35          [-1, 128, 28, 28]               0\n","           Conv2d-36          [-1, 128, 28, 28]          16,384\n","      BatchNorm2d-37          [-1, 128, 28, 28]             256\n","             ReLU-38          [-1, 128, 28, 28]               0\n","           Conv2d-39          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-40          [-1, 128, 28, 28]             256\n","             ReLU-41          [-1, 128, 28, 28]               0\n","           Conv2d-42          [-1, 512, 28, 28]          65,536\n","      BatchNorm2d-43          [-1, 512, 28, 28]           1,024\n","             ReLU-44          [-1, 512, 28, 28]               0\n","           Conv2d-45          [-1, 128, 28, 28]          65,536\n","      BatchNorm2d-46          [-1, 128, 28, 28]             256\n","             ReLU-47          [-1, 128, 28, 28]               0\n","           Conv2d-48          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-49          [-1, 128, 28, 28]             256\n","             ReLU-50          [-1, 128, 28, 28]               0\n","           Conv2d-51          [-1, 512, 28, 28]          65,536\n","      BatchNorm2d-52          [-1, 512, 28, 28]           1,024\n","             ReLU-53          [-1, 512, 28, 28]               0\n","           Conv2d-54          [-1, 128, 28, 28]          65,536\n","      BatchNorm2d-55          [-1, 128, 28, 28]             256\n","             ReLU-56          [-1, 128, 28, 28]               0\n","           Conv2d-57          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-58          [-1, 128, 28, 28]             256\n","             ReLU-59          [-1, 128, 28, 28]               0\n","           Conv2d-60          [-1, 512, 28, 28]          65,536\n","      BatchNorm2d-61          [-1, 512, 28, 28]           1,024\n","             ReLU-62          [-1, 512, 28, 28]               0\n","           Conv2d-63          [-1, 128, 28, 28]          65,536\n","      BatchNorm2d-64          [-1, 128, 28, 28]             256\n","             ReLU-65          [-1, 128, 28, 28]               0\n","           Conv2d-66          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-67          [-1, 128, 28, 28]             256\n","             ReLU-68          [-1, 128, 28, 28]               0\n","           Conv2d-69          [-1, 512, 28, 28]          65,536\n","      BatchNorm2d-70          [-1, 512, 28, 28]           1,024\n","           Conv2d-71          [-1, 512, 28, 28]          65,536\n","             ReLU-72          [-1, 512, 28, 28]               0\n","            Block-73          [-1, 512, 28, 28]               0\n","           Conv2d-74          [-1, 256, 14, 14]       1,179,648\n","      BatchNorm2d-75          [-1, 256, 14, 14]             512\n","             ReLU-76          [-1, 256, 14, 14]               0\n","            Block-77          [-1, 256, 14, 14]               0\n","           Conv2d-78          [-1, 256, 14, 14]          65,536\n","      BatchNorm2d-79          [-1, 256, 14, 14]             512\n","             ReLU-80          [-1, 256, 14, 14]               0\n","           Conv2d-81          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-82          [-1, 256, 14, 14]             512\n","             ReLU-83          [-1, 256, 14, 14]               0\n","           Conv2d-84         [-1, 1024, 14, 14]         262,144\n","      BatchNorm2d-85         [-1, 1024, 14, 14]           2,048\n","             ReLU-86         [-1, 1024, 14, 14]               0\n","           Conv2d-87          [-1, 256, 14, 14]         262,144\n","      BatchNorm2d-88          [-1, 256, 14, 14]             512\n","             ReLU-89          [-1, 256, 14, 14]               0\n","           Conv2d-90          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-91          [-1, 256, 14, 14]             512\n","             ReLU-92          [-1, 256, 14, 14]               0\n","           Conv2d-93         [-1, 1024, 14, 14]         262,144\n","      BatchNorm2d-94         [-1, 1024, 14, 14]           2,048\n","             ReLU-95         [-1, 1024, 14, 14]               0\n","           Conv2d-96          [-1, 256, 14, 14]         262,144\n","      BatchNorm2d-97          [-1, 256, 14, 14]             512\n","             ReLU-98          [-1, 256, 14, 14]               0\n","           Conv2d-99          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-100          [-1, 256, 14, 14]             512\n","            ReLU-101          [-1, 256, 14, 14]               0\n","          Conv2d-102         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-103         [-1, 1024, 14, 14]           2,048\n","            ReLU-104         [-1, 1024, 14, 14]               0\n","          Conv2d-105          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-106          [-1, 256, 14, 14]             512\n","            ReLU-107          [-1, 256, 14, 14]               0\n","          Conv2d-108          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-109          [-1, 256, 14, 14]             512\n","            ReLU-110          [-1, 256, 14, 14]               0\n","          Conv2d-111         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-112         [-1, 1024, 14, 14]           2,048\n","            ReLU-113         [-1, 1024, 14, 14]               0\n","          Conv2d-114          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-115          [-1, 256, 14, 14]             512\n","            ReLU-116          [-1, 256, 14, 14]               0\n","          Conv2d-117          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-118          [-1, 256, 14, 14]             512\n","            ReLU-119          [-1, 256, 14, 14]               0\n","          Conv2d-120         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-121         [-1, 1024, 14, 14]           2,048\n","            ReLU-122         [-1, 1024, 14, 14]               0\n","          Conv2d-123          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-124          [-1, 256, 14, 14]             512\n","            ReLU-125          [-1, 256, 14, 14]               0\n","          Conv2d-126          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-127          [-1, 256, 14, 14]             512\n","            ReLU-128          [-1, 256, 14, 14]               0\n","          Conv2d-129         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-130         [-1, 1024, 14, 14]           2,048\n","          Conv2d-131         [-1, 1024, 14, 14]         262,144\n","            ReLU-132         [-1, 1024, 14, 14]               0\n","           Block-133         [-1, 1024, 14, 14]               0\n","          Conv2d-134            [-1, 512, 7, 7]       4,718,592\n","     BatchNorm2d-135            [-1, 512, 7, 7]           1,024\n","            ReLU-136            [-1, 512, 7, 7]               0\n","           Block-137            [-1, 512, 7, 7]               0\n","          Conv2d-138            [-1, 512, 7, 7]         262,144\n","     BatchNorm2d-139            [-1, 512, 7, 7]           1,024\n","            ReLU-140            [-1, 512, 7, 7]               0\n","          Conv2d-141            [-1, 512, 7, 7]       2,359,296\n","     BatchNorm2d-142            [-1, 512, 7, 7]           1,024\n","            ReLU-143            [-1, 512, 7, 7]               0\n","          Conv2d-144           [-1, 2048, 7, 7]       1,048,576\n","     BatchNorm2d-145           [-1, 2048, 7, 7]           4,096\n","            ReLU-146           [-1, 2048, 7, 7]               0\n","          Conv2d-147            [-1, 512, 7, 7]       1,048,576\n","     BatchNorm2d-148            [-1, 512, 7, 7]           1,024\n","            ReLU-149            [-1, 512, 7, 7]               0\n","          Conv2d-150            [-1, 512, 7, 7]       2,359,296\n","     BatchNorm2d-151            [-1, 512, 7, 7]           1,024\n","            ReLU-152            [-1, 512, 7, 7]               0\n","          Conv2d-153           [-1, 2048, 7, 7]       1,048,576\n","     BatchNorm2d-154           [-1, 2048, 7, 7]           4,096\n","            ReLU-155           [-1, 2048, 7, 7]               0\n","          Conv2d-156            [-1, 512, 7, 7]       1,048,576\n","     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n","            ReLU-158            [-1, 512, 7, 7]               0\n","          Conv2d-159            [-1, 512, 7, 7]       2,359,296\n","     BatchNorm2d-160            [-1, 512, 7, 7]           1,024\n","            ReLU-161            [-1, 512, 7, 7]               0\n","          Conv2d-162           [-1, 2048, 7, 7]       1,048,576\n","     BatchNorm2d-163           [-1, 2048, 7, 7]           4,096\n","          Conv2d-164           [-1, 2048, 7, 7]       1,048,576\n","            ReLU-165           [-1, 2048, 7, 7]               0\n","           Block-166           [-1, 2048, 7, 7]               0\n","       AvgPool2d-167           [-1, 2048, 7, 7]               0\n","         Flatten-168               [-1, 100352]               0\n","          Linear-169                    [-1, 1]         100,353\n","         Sigmoid-170                    [-1, 1]               0\n","================================================================\n","Total params: 28,075,201\n","Trainable params: 28,075,201\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 223.59\n","Params size (MB): 107.10\n","Estimated Total Size (MB): 331.27\n","----------------------------------------------------------------\n"]}],"source":["torchsummary.summary(model, (3, 224, 224), device='cuda')"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"O61rIk7bHNU3"},"outputs":[],"source":["input_names = ['Input']\n","output_names = ['Output']\n","\n","x = torch.zeros(2, 3, 224, 224).to(device)\n","torch.onnx.export(model, x, 'Resnet50.onnx', input_names=input_names, output_names=output_names)"]},{"cell_type":"markdown","metadata":{"id":"dmkPPC4MHNU3"},"source":["<img src='https://drive.google.com/uc?export=download&id=1jcBl2ujLBCpK_8KXMMksQYGiNGs5KJNb' width=\"\" height =\"\" /><br>"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"going_deeper1.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.12 ('pytorch')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"125b91887fe90942a6257b42f930dad6e8c6560c7d21a8c65ae0fa0536e0ec6e"}}},"nbformat":4,"nbformat_minor":0}
